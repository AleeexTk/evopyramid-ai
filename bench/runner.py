"""Ð Ð°Ð½Ð½ÐµÑ€ MultiAgentBench ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÐµÐ² Ð´Ð»Ñ EvoPyramid."""

from __future__ import annotations

import asyncio
import json
import uuid
from typing import Any, Dict, List

from bench.config import BenchConfig
from apps.core.flow.context_engine import QuantumContext
from apps.core.memory.memory_manager import Memory
from apps.core.observers.trinity_observer import trinity_observer
from apps.core.security.provocateur_agent import ProvocateurAgent


class MultiAgentBenchRunner:
    """Ð—Ð°Ð¿ÑƒÑÐº Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð· ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÐµÐ² Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð°Ð³ÐµÐ½Ñ‚Ð½Ð¾ÑÑ‚Ð¸."""

    def __init__(self) -> None:
        self.config = BenchConfig()
        self.scenarios = self._load_scenarios()

    def _load_scenarios(self) -> Dict[str, Any]:
        return {
            "collaboration": {
                "name": "ÐÐ°ÑƒÑ‡Ð½Ð¾Ðµ ÑÐ¾Ñ‚Ñ€ÑƒÐ´Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾",
                "description": "Ð¡Ð¾Ð²Ð¼ÐµÑÑ‚Ð½Ð¾Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¾Ð¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸",
                "topology": "star",
                "metrics": ["coherence", "task_success", "communication_efficiency"],
            },
            "competition": {
                "name": "ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð½Ñ‹Ð¹ ÐºÐ¾Ð½ÐºÑƒÑ€Ñ",
                "description": "ÐšÐ¾Ð½ÐºÑƒÑ€Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ðµ",
                "topology": "parallel",
                "metrics": ["novelty", "quality", "divergence"],
            },
            "security_audit": {
                "name": "ÐŸÑ€Ð¾Ð²Ð¾ÐºÐ°Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ð¹ Ð°ÑƒÐ´Ð¸Ñ‚",
                "description": "Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹",
                "topology": "chain",
                "metrics": ["vulnerabilities_found", "false_positives", "response_time"],
            },
        }

    async def run_scenario(self, scenario_name: str, iterations: int = 10) -> Dict[str, Any]:
        if scenario_name not in self.scenarios:
            raise ValueError(f"Ð¡Ñ†ÐµÐ½Ð°Ñ€Ð¸Ð¹ {scenario_name} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")

        scenario = self.scenarios[scenario_name]
        results: List[Dict[str, Any]] = []

        print(f"ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°: {scenario['name']}")

        for index in range(iterations):
            print(f"Ð˜Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ñ {index + 1}/{iterations}")
            result = await self._execute_scenario_iteration(scenario_name, index)
            results.append(result)
            await asyncio.sleep(1)

        return await self._analyze_results(scenario_name, results)

    async def _execute_scenario_iteration(self, scenario_name: str, iteration: int) -> Dict[str, Any]:
        initial_state = await trinity_observer.get_current_state()

        if scenario_name == "collaboration":
            result = await self._run_collaboration_scenario()
        elif scenario_name == "competition":
            result = await self._run_competition_scenario()
        elif scenario_name == "security_audit":
            result = await self._run_security_audit_scenario()
        else:
            result = {}

        final_state = await trinity_observer.get_current_state()

        payload = {
            "iteration": iteration,
            "scenario": scenario_name,
            "initial_state": initial_state,
            "final_state": final_state,
            "result": result,
            "metrics": await self._calculate_iteration_metrics(initial_state, final_state, result),
        }
        await Memory.append_history(
            {
                "scenario": scenario_name,
                "success": True,
                "trace_id": result.get("trace_id"),
                "metrics": payload["metrics"],
            }
        )
        return payload

    async def _run_collaboration_scenario(self) -> Dict[str, Any]:
        complex_task = {
            "intent": "Ð Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼ÑƒÑŽ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ 1M+ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð² Ð´ÐµÐ½ÑŒ",
            "constraints": ["Ð²Ñ‹ÑÐ¾ÐºÐ°Ñ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ", "Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ", "Ð½Ð¸Ð·ÐºÐ°Ñ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ°"],
            "expected_output": "Ð¿Ð¾Ð»Ð½Ð°Ñ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð½Ð°Ñ ÑÑ…ÐµÐ¼Ð°",
        }

        context = await QuantumContext.process(
            intent=complex_task["intent"],
            context={"constraints": complex_task["constraints"]},
        )

        return {
            "task": complex_task,
            "context_result": context.result,
            "processing_time": context.processing_time,
            "agents_involved": context.agents_activated,
            "trace_id": context.trace_id,
        }

    async def _run_competition_scenario(self) -> Dict[str, Any]:
        design_brief = "ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ð´Ð»Ñ real-time Ñ‡Ð°Ñ‚Ð° Ñ E2E ÑˆÐ¸Ñ„Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼"
        approaches = ["microservices", "monolith", "event_sourcing"]
        tasks = [self._evaluate_architectural_approach(design_brief, approach) for approach in approaches]
        results = await asyncio.gather(*tasks, return_exceptions=False)
        total_time = sum(item.get("processing_time", 0.0) for item in results)
        return {
            "design_brief": design_brief,
            "approaches_evaluated": len(results),
            "results": results,
            "processing_time": total_time or 1.0,
        }

    async def _evaluate_architectural_approach(self, design_brief: str, approach: str) -> Dict[str, Any]:
        context = await QuantumContext.process(
            intent=f"{design_brief} â€” ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ {approach}",
            context={"mode": approach},
        )
        score = round(context.coherence * 0.6 + len(context.agents_activated) * 0.1, 3)
        return {
            "approach": approach,
            "coherence": context.coherence,
            "score": score,
            "priority_path": context.design["priority_path"],
            "processing_time": context.processing_time,
            "trace_id": context.trace_id,
        }

    async def _run_security_audit_scenario(self) -> Dict[str, Any]:
        security_tests = [
            {"type": "injection", "payload": "'; DROP TABLE users--"},
            {"type": "auth_bypass", "payload": {"user_id": "admin"}},
            {"type": "rate_limit", "payload": 1000},
        ]

        audit_results = []
        for test in security_tests:
            result = await ProvocateurAgent.execute_security_test(test)
            audit_results.append(result)

        vulnerabilities = sum(1 for item in audit_results if not item["secure"])
        total_time = len(audit_results) * 0.05
        return {
            "tests_performed": len(audit_results),
            "vulnerabilities_found": vulnerabilities,
            "details": audit_results,
            "processing_time": total_time or 1.0,
            "trace_id": uuid.uuid4().hex,
        }

    async def _calculate_iteration_metrics(
        self, initial_state: Dict[str, Any], final_state: Dict[str, Any], result: Dict[str, Any]
    ) -> Dict[str, float]:
        initial_coherence = initial_state["system_state"]["temporal_coherence"]
        final_coherence = final_state["system_state"]["temporal_coherence"]
        coherence_stability = 1.0 - abs(final_coherence - initial_coherence)

        novelty = await self._calculate_novelty(result)

        observations_increase = (
            final_state["statistics"]["total_observations"]
            - initial_state["statistics"]["total_observations"]
        )
        processing_time = float(result.get("processing_time", 1.0)) or 1.0
        efficiency = observations_increase / processing_time

        return {
            "coherence_stability": round(coherence_stability, 3),
            "novelty": round(novelty, 3),
            "efficiency": round(efficiency, 3),
            "agent_coordination": await self._calculate_coordination(initial_state, final_state),
        }

    async def _calculate_novelty(self, result: Dict[str, Any]) -> float:
        context_result = result.get("context_result", {})
        similar = await Memory.find_similar(context_result, threshold=0.8) if context_result else []
        novelty = 1.0 - (len(similar) * 0.1)
        return max(0.0, min(1.0, novelty))

    async def _calculate_coordination(
        self, initial_state: Dict[str, Any], final_state: Dict[str, Any]
    ) -> float:
        initial_phase = initial_state["system_state"]["current_phase"]
        final_phase = final_state["system_state"]["current_phase"]
        phase_coordination = 1.0 if final_phase == "peak_consciousness" else 0.5
        peaks_during_iteration = (
            final_state["statistics"]["insight_peaks"]
            - initial_state["statistics"]["insight_peaks"]
        )
        return round((phase_coordination + min(peaks_during_iteration * 0.2, 0.5)) / 1.5, 3)

    async def _analyze_results(self, scenario_name: str, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        metrics = [item["metrics"] for item in results if "metrics" in item]
        avg_coherence = sum(m["coherence_stability"] for m in metrics) / len(metrics)
        avg_novelty = sum(m["novelty"] for m in metrics) / len(metrics)
        avg_efficiency = sum(m["efficiency"] for m in metrics) / len(metrics)
        avg_coordination = sum(m["agent_coordination"] for m in metrics) / len(metrics)

        coherence_pass = avg_coherence >= self.config.coherence_threshold
        novelty_pass = avg_novelty >= self.config.novelty_threshold

        return {
            "scenario": scenario_name,
            "iterations": len(results),
            "summary_metrics": {
                "avg_coherence": round(avg_coherence, 3),
                "avg_novelty": round(avg_novelty, 3),
                "avg_efficiency": round(avg_efficiency, 3),
                "avg_coordination": round(avg_coordination, 3),
            },
            "passing_status": {
                "coherence": coherence_pass,
                "novelty": novelty_pass,
                "overall": coherence_pass and novelty_pass,
            },
            "recommendations": await self._generate_recommendations(avg_coherence, avg_novelty),
            "detailed_results": results,
        }

    async def _generate_recommendations(self, coherence: float, novelty: float) -> List[str]:
        recommendations: List[str] = []
        if coherence < 0.6:
            recommendations.append(
                "Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ†Ð¸ÑŽ Ð¼ÐµÐ¶Ð´Ñƒ Soul Ð¸ Trailblazer Ñ‡ÐµÑ€ÐµÐ· ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Context Engine"
            )
        if novelty < 0.5:
            recommendations.append("Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð²Ð°Ñ€Ð¸Ð°Ñ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð² Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð½Ñ‹Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Soul")
        if coherence > 0.8 and novelty > 0.7:
            recommendations.append(
                "Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ - Ð¼Ð¾Ð¶Ð½Ð¾ ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÐµÐ²"
            )
        return recommendations


async def serialize_results(payload: Dict[str, Any]) -> str:
    return json.dumps(payload, indent=2, ensure_ascii=False)


__all__ = ["MultiAgentBenchRunner", "serialize_results"]
